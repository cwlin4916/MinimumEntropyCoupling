import os
import torch
import numpy as np
from contextlib import nullcontext
from sklearn.model_selection import train_test_split
import wandb
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
from torch.optim import Adam
import torch.nn.functional as F
from src.mec_model import DistributionTransformerModel  # Ensure correct import path
from datetime import datetime

# Configuration and Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
out_dir = "output"
os.makedirs(out_dir, exist_ok=True)

# Hyperparameters
config = {
    'epochs': 1000,
    'eval_interval': 40,
    'learning_rate': 1e-4,
    'batch_size': 64,
    'gradient_accumulation_steps': 5 * 8,
    'dataset_size': 3,
    'distribution_size': 3,
    #dataset_path would be the path to the data generated by the data_gen_mecv1.py, this would be determined by dataset_size, distirbution_size 
    'dataset_path': "/Users/miltonlin/Documents/GitHub/MinimumEntropyCoupling/src/data/mec_generation/mec_data_3_10^3.pth",
    'project_name': 'MEC_learning1',
    'entity': 'cwlin416',
    'checkpoint_path':  "/Users/miltonlin/Documents/GitHub/MinimumEntropyCoupling/src/output/model_checkpoint.pth.tar",
    'init_from': 'scratch', # 'scratch' or 'resume' or 'model*' 
    'loss': 'MSE'
}

# Function to save checkpoint
def save_checkpoint(state, filename="checkpoint.pth.tar"):
    torch.save(state, filename)

def load_checkpoint(filename):
    if os.path.isfile(filename):
        checkpoint = torch.load(filename)
        print("Loaded checkpoint '{}' (epoch {})".format(filename, checkpoint['epoch']))
        return checkpoint
    else:
        print("No checkpoint found at '{}'".format(filename))
        return None
 
def load_data(filename, test_size=0.2, batch_size=64):
    print("loading data...")
    data = torch.load(filename)
    
    # Convert list of tuples into separate lists for p, q, and M
    p_list = [torch.tensor(item[0], dtype=torch.float32).unsqueeze(1) for item in data]
    q_list = [torch.tensor(item[1], dtype=torch.float32).unsqueeze(1) for item in data]
    M_list = [torch.tensor(item[2], dtype=torch.float32) for item in data]
    
    # Stack to create tensors
    p_tensor = torch.stack(p_list)
    q_tensor = torch.stack(q_list)
    M_tensor = torch.stack(M_list)

    print("Shapes after stacking:")
    print("p_tensor:", p_tensor.size())  # Should be [1000, 3, 1]
    print("q_tensor:", q_tensor.size())  # Should be [1000, 3, 1]
    print("M_tensor:", M_tensor.size())  # Check the shape, should typically be [1000, m, n]

    # Split the indices into training and test sets
    train_idx, test_idx = train_test_split(range(len(M_tensor)), test_size=test_size, random_state=42)
    
    # Create training and test datasets
    train_dataset = TensorDataset(p_tensor[train_idx], q_tensor[train_idx], M_tensor[train_idx])
    test_dataset = TensorDataset(p_tensor[test_idx], q_tensor[test_idx], M_tensor[test_idx])
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print("Successfully loaded data!")
    return train_loader, test_loader

# Plot function to save figure 
def plot_losses(train_losses, test_losses,k,m,e,loss):
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(test_losses, label='Test Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    #save figure inlcudes k,m,e and type of loss 
    plt.savefig(f"output/loss_curve_{k}_{m}_{e}_loss{loss}.png") # save the figure to the directory 
    # plt.show()
    # save figure with the appropriate name with dataset_size, and distribution_size 
    #save the figure to the directory
    
    wandb.log({"Loss Curves": wandb.Image(plt)})
    plt.close()
    
#checkpoint saving function 
def save_checkpoint(state, filename="checkpoint.pth"):
    torch.save(state, filename)

# Evaluation function
def evaluate(model, test_loader):
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for row_sums, col_sums, matrices in test_loader:
            row_sums, col_sums, matrices = row_sums.to(device), col_sums.to(device), matrices.to(device)
            outputs = model(row_sums, col_sums)
            val_loss = F.mse_loss(outputs, matrices)
            total_val_loss += val_loss.item()
    average_val_loss = total_val_loss / len(test_loader)
    return average_val_loss



# Evaluation function
def evaluate(model, test_loader):
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for p, q, M in test_loader:
            p, q, M = p.to(device), q.to(device), M.to(device)
            outputs = model(p, q)
            val_loss = F.mse_loss(outputs, M)
            total_val_loss += val_loss.item()
    return total_val_loss / len(test_loader)

# Training function
def train(model, train_loader, test_loader, epochs, optimizer, checkpoint_path, init_from):
    print(f"Training on {device}")
    start_epoch = 0
    best_val_loss = float('inf')
    if init_from == 'resume':
        checkpoint = load_checkpoint(checkpoint_path)
        if checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state'])
            start_epoch = checkpoint['epoch']
            best_val_loss = checkpoint['best_val_loss']
    
    wandb.init(project=config['project_name'], entity=config['entity'])
    train_losses, test_losses = [], []

    for epoch in range(start_epoch, epochs):
        model.train()
        train_loss = 0
        for p, q, M in train_loader:
            p, q, M = p.to(device), q.to(device), M.to(device)
            optimizer.zero_grad()
            output = model(p, q)
            loss = F.mse_loss(output, M)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        test_loss = evaluate(model, test_loader)
        train_losses.append(avg_train_loss)
        test_losses.append(test_loss)

        wandb.log({"epoch": epoch, "train_loss": avg_train_loss, "test_loss": test_loss})
        print(f"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}, Time: {datetime.now()}")

        if test_loss < best_val_loss or epoch % config['eval_interval'] == 0:
            best_val_loss = min(best_val_loss, test_loss)
            save_checkpoint({
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
            }, filename=checkpoint_path)
    k= config['dataset_size']
    m= config['distribution_size']
    e= config['epochs']
    loss = config['loss']
    plot_losses(train_losses, test_losses,k,m,e,loss)

# Main
if __name__ == "__main__":
    config['dataset_size'] = 4
    config['epochs']=700
    k= config['dataset_size']
    e= config['epochs']
    
    for m in range(3, 5):
        config['distribution_size'] = m
        config['dataset_path'] = f"data/mec_generation/mec_data_{m}_10^{k}.pth"
        
        config['checkpoint_path'] = f"output/model_checkpoint_{k}_{m}_{e}.pth"
        train_loader, test_loader = load_data(config['dataset_path'])
        model = DistributionTransformerModel(input_dim=1, model_dim=512, num_heads=8, num_layers=3).to(device)
        optimizer = Adam(model.parameters(), lr=config['learning_rate'])
        train(model, train_loader, test_loader, config['epochs'], optimizer, config['checkpoint_path'], config['init_from'])
        if 'RANK' in os.environ:
            destroy_process_group()